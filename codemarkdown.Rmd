---
title: "Coding"
author: "Hugh Warden"
date: "19/04/2021"
output: 
    html_document:
      toc: true
      toc_float: true
      number_sections: true
      theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Introduction

# ConstructionCode

This script is used to read in protein interaction data and convert it into a form which can be utilised to calculate betweenness centality.

Firstly, the igraph package is loaded and the working directory is set

```{r}
library(igraph)
setwd("/Users/hugh/Documents/University/Maths/Year4/Project/RCode/CentralityCode")
```

Then the mips interaction data set is imported and processed

```{r}
mips.full <- read.csv("Data/mips.txt", sep = "\t")
mips.full <- mips.full[1:217,]
mips.edgelist <- mips.full[,1:2]
```

The reason only the first 217 entries of the mis data is to make this feasible to run on a laptop with low processing power. The more powerful the machine running this code, the more entries should be used.

Then the full yeast interactome is loaded

```{r}
ppi.full <- as.data.frame(read.csv("/Users/hugh/Documents/University/Maths/Year4/Project/RCode/BIOGRID-ORGANISM-Saccharomyces_cerevisiae_S288c-4.3.195.tab3.txt", sep = "\t"))
ppi.edgelist <- ppi.full[,6:7]
```

This data is not available on this github repository as it is too large, but it can be downloaded from the BIOGRID website.

Then the proteins involved in the mips and ppi data sets are found. If the computer is powerful enough, the whole yeast interactome should be analysed. Otherwise, the mips data set will select important proteins that can also be linked back to annotated complexes.

```{r}
mips.proteins <- unique(c(mips.edgelist[,1], mips.edgelist[,2]))
ppi.proteins <- unique(c(ppi.edgelist[,1], ppi.edgelist[,2]))
ppi.edgelist <- unique(ppi.edgelist[which((ppi.edgelist[,1] %in% mips.proteins)&(ppi.edgelist[,2] %in% mips.proteins)),])
```

This edgelist is now used to make a graph object. This is then parsed as a matrix, that is manipulated to simplify the graph.

```{r}
ppi.graph <- graph_from_edgelist(as.matrix(ppi.edgelist), directed = FALSE)
ppi.adj_mat <- as.matrix(as_adjacency_matrix(ppi.graph))
rnames <- rownames(ppi.adj_mat)
ppi.adj_mat <- matrix(as.numeric(ppi.adj_mat > 0), ncol = ncol(ppi.adj_mat))
diag(ppi.adj_mat) <- 0
ppi.graph <- graph_from_adjacency_matrix(ppi.adj_mat, mode = "undirected", diag = FALSE)
```

Then all of the maximal cliques within the graph are identified.

```{r}
ppi.cliques <- max_cliques(ppi.graph, min = 2)
```

Each of these maximal cliques is then assumed to be a protein complex, and therefore modelled as a hyperedge.

```{r}
ppi.inc_mat <- matrix(0, nrow = length(rnames), ncol = length(ppi.cliques))
rownames(ppi.inc_mat) <- rnames
for (i in 1:length(ppi.cliques)){
  ppi.inc_mat[ppi.cliques[[i]],i] <- 1
}
```

Given a more detailed dataset, these cliques would not need to be found. The protein complex annotations should be used to populate the incidence matrix.

This data is then saved to be used to calculate betweenness centralities.

```{r}
save(ppi.graph, ppi.inc_mat, file = "Data/Constructions.RData")
```

# HypergraphBC

This is a python script containing functions to be imported into R via the reticulate packages. It calculates the dual hypergraph adjacency matrix and three types of betweenness centrality for hypergraphs.

Firstly, the required packages are imported

```{python}
import networkx as nx
import numpy as np
```

## Hypergraph Dual Adjacency Matrix

Then, there is a function to calculate the dual hypergraph adjacency matrix via the incidence matrix

```{python}
def get_hyperedge_connectivity_adj_mat(inc_mat):
    tinc_mat = np.transpose(inc_mat)
    ret_mat = tinc_mat @ inc_mat
    return np.matrix(ret_mat)
```

This calculates the dual adjacency matrix in terms of the icindence matrix. It is processed later to binarise it and remove the leading diagonal.

## Hypergraph Betweenness Centrality

Then, the hypergraph dual adjacency matrix and the incidence matrix are used as parameters in a function to calculate betweenness centrality for hypergraphs.

Firstly, the function is defined and the required variables are initialised

```{python}
def get_hypergraph_betweenness(adj_mat, inc_mat):
    g = nx.convert_matrix.from_numpy_matrix(adj_mat)
    n_vert = adj_mat.shape[0]
    m_vert = inc_mat.shape[0]
    bc_all = np.zeros(m_vert)
    bc_nt = np.zeros(m_vert)
    bc_act = np.zeros(m_vert)
```

g is the support graph of the adjacency matrix. n_vert is the number of hyperedges of the hypergraph (or the number of vertices in the dual support graph). m_vert is the the number of vertices in the hypergraph. bc_* are then vectors containing the betweenness centrality of the vertices, with bc_all being active and passive members, bc_nt being active and non-terminating passive members and bc_act being active members.

Then each pair of vertices is selected

```{python}
for i in range(m_vert-1):
    for j in range(i+1, m_vert):
```

and for each pair of vertices the following is calculated.

Temporary update variables are created for each betweenness centrality

```{python}
temp_bc_all = np.zeros(m_vert)
temp_bc_nt = np.zeros(m_vert)
temp_bc_act = np.zeros(m_vert)
```

Then a temporary copy of the dual support graph is created and has two vertices added

```{python}
temp = g.copy()
temp.add_node(n_vert+1)
temp.add_node(n_vert+2)
```

This pair of vertices are modelling the pairs of vertices being iterated over. Each of these vertices are then connected to vertices of the dual support graph if the modelled vertices and hyperedges are incident in the original hypergraph.

```{python}
for k in range(n_vert):
    if inc_mat[i,k] == 1:
        temp.add_edge(n_vert+1, k)
for k in range(n_vert):
    if inc_mat[j,k] == 1:
        temp.add_edge(k,n_vert+2)
```

Then all of the shortest paths between these two new vertices are calculated and iterated over. For each path, the hyperpath induced subhypergraphis calculated by selecting all columns of the incidence matrix that represent hyperedges from the path. A row sum is then used to identify degree in this subhypergraph to ascertain membership type (an extra check is made to see if passive members are non-terminating). Then the relevant entries of the update variables are incremented.

```{python}
count = 0
for p in nx.all_shortest_paths(temp, n_vert+1, n_vert+2):
    temp_inc_mat = inc_mat[:,p[1:(len(p)-1)]]
    r_sum = np.squeeze(np.asarray(temp_inc_mat.sum(1)))
    temp_bc_all[np.where(r_sum > 0)] += 1
    temp_bc_act[np.where(r_sum > 1)] += 1
    if len(p) > 4:
        temp_inc_mat = inc_mat[:,p[2:(len(p)-2)]]
        r_sum = np.squeeze(np.asarray(temp_inc_mat.sum(1)))
        temp_bc_nt[np.where(r_sum > 0)] += 1
    else:
        temp_bc_nt[np.where(r_sum > 0)] += 1
    count += 1
```

The count is increased every time and if the count is positive, the betweenness centrality updates are divided by count to give proportions, the update values of path endpoints are zeroed and then the betweenness centrality variables are updated.

```{python}
if count > 0:
                temp_bc_all = temp_bc_all / count
                temp_bc_nt = temp_bc_nt / count
                temp_bc_act = temp_bc_act / count
                
                temp_bc_all[i] = 0
                temp_bc_all[j] = 0
                temp_bc_nt[i] = 0
                temp_bc_nt[j] = 0
                temp_bc_act[i] = 0
                temp_bc_act[j] = 0
            
                bc_all += temp_bc_all
                bc_nt += temp_bc_nt
                bc_act += temp_bc_act
```

These betweenness centrality values are then returned as a list.

```{python}
return bc_all, bc_nt, bc_act
```

# BetweennessCode

This script calculates all versions of betweenness centrality and saves them.

Firstly, the required packages are loaded

```{r}
library(igraph)
library(reticulate)
```

Then, the "default" virtual environment is loaded for reticulate, this already has the numpy and networkx packages installed on it. Also, the HypergraphBC code from above is loaded into the environment, allowing access to both functions from the R environment.

```{r}
use_virtualenv("default")
source_python("/Users/hugh/Documents/University/Maths/Year4/Project/RCode/CentralityCode/HypergraphBC.py")
```

The data previously calculated is loaded

```{r}
load("/Users/hugh/Documents/University/Maths/Year4/Project/RCode/CentralityCode/Data/Constructions.RData")
```

The python code to calculate the dual adjacency matrix is called. The result is then binarised and loops are removed.

```{r}
hypergaph.dual_adj_mat <- get_hyperedge_dual_adj_mat(ppi.inc_mat)
hypergaph.dual_adj_mat <- matrix(as.numeric(hypergaph.dual_adj_mat > 0), ncol = ncol(hypergaph.dual_adj_mat))
diag(hypergaph.dual_adj_mat) <- 0
```

Then, the python code to calculate all forms of betweenness centrality is called and so is the traditional graph betweenness centrality.

```{r}
hypergraph.bc <- get_hypergraph_betweenness(hypergaph.dual_adj_mat, ppi.inc_mat)
names(hypergraph.bc) <- c("all", "nt", "act")
graph.bc <- betweenness(ppi.graph)
```

All pertinent data is then collected in a data frame and saved

```{r}
bc.data <- data.frame(protein = rownames(ppi.inc_mat), gdeg = degree(ppi.graph), ncomp = apply(ppi.inc_mat, 1, sum),graph = graph.bc, hall = hypergraph.bc$all, hnt = hypergraph.bc$nt, hact = hypergraph.bc$act)
save(bc.data, file = "/Users/hugh/Documents/University/Maths/Year4/Project/RCode/CentralityCode/Data/Betweenness.RData")
```






